# å°å…¥å¿…è¦å¥—ä»¶
import pymysql
import pandas as pd
import numpy as np
from pathlib import Path
import joblib  # ç”¨ä»¥ä¿å­˜/è¼‰å…¥æ¨¡å‹
import json
from datetime import datetime, timedelta

# è¼‰å…¥airflowæ‰€éœ€å¥—ä»¶
import pendulum
from airflow.decorators import dag, task
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

# ===è¨­å®šMySQLè³‡è¨Šã€æ¨¡å‹æª”æ¡ˆå­˜æ”¾ä½ç½®

# æ¨¡å‹å„²å­˜è·¯å¾‘è¨­å®š
model_dir = "/app/models"
model_path = f"{model_dir}/fruit_price_model.pkl"
metadata_path = f"{model_dir}/model_metadata.json"
feature_cols_path = f"{model_dir}/feature_columns.pkl"
crop_index_path = f"{model_dir}/crop_index_mapping.pkl"

# æ¨¡å‹åƒæ•¸è¨­å®š
defaults = {
    "train_days": 365,  # æœ€å¤§å˜—è©¦æŠ“çš„è¨“ç·´å¤©æ•¸
    "max_train_days": 730,  # è¨“ç·´æœ€å¤šä¸è¶…é 730 å¤©
    "test_days": 90,  # æ¸¬è©¦é›†å¤©æ•¸ï¼ˆå¾ 90 é™åˆ° 30ï¼Œæ›´å®¹æ˜“æ»¿è¶³ï¼‰
    "validation_windows": 15,  # TimeSeriesSplit çš„åˆ‡æ³•ï¼ˆå¾ 15 é™åˆ° 10ï¼‰
    "forecast_horizon": 8,  # æœªä¾†é æ¸¬å¤©æ•¸
    "min_data_points": 60,  # æœ€å°è³‡æ–™é»æ•¸ï¼ˆæ¸¬è©¦é›† 30 + è¨“ç·´é›†è‡³å°‘ 30ï¼‰
}


# è¨­å®šè³‡æ–™åº«é€£ç·šè³‡è¨Š
db_config = {
    "host": "104.199.220.12",
    "port": 3306,
    "user": "tjr103-team02",
    "passwd": "password",
    "db": "tjr103-team02",
    "charset": "utf8mb4",
}

# è¨­å®šè³‡æ–™åº«æŸ¥è©¢æ¨¡æ¿
sql_template = """
    SELECT
        `date`
        ,crop_id
        ,crop_price_per_kg
        ,is_typhoon
        ,typhoon_name
        ,station_pressure_area AS station_pressure
        ,air_temperature_area AS air_temperature
        ,relative_humidity_area AS relative_humidity
        ,wind_speed_area AS wind_speed
        ,precipitation_area AS precipitation
    FROM v_crop_daily
    WHERE `date` >= CURDATE() - INTERVAL 365*2 DAY
"""

# ===================================è¨­å®šè³‡è¨Šçš„çµå°¾===================================


# ===å®šç¾©å¾MySQL Serverå–å¾—è³‡æ–™çš„å‡½æ•¸
def query_data_from_mysql(sql_template: str):
    """å¾ MySQL è®€å– v_crop_daily view"""

    try:
        with pymysql.connect(**db_config) as connection:

            # æº–å‚™ SQL æŸ¥è©¢
            print("é–‹å§‹åŸ·è¡Œ select...")

            # åŸ·è¡ŒæŸ¥è©¢
            df = pd.read_sql(sql_template, connection)

            print(f"æˆåŠŸå–å¾— {(df.shape[0])} ç­†è³‡æ–™ã€‚")

    except Exception as e:
        print(f"è³‡æ–™åº«æ“ä½œç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        raise

    # æª¢æŸ¥ DataFrame æ˜¯å¦ç‚ºç©º
    if df.empty:
        print("æŸ¥ç„¡è³‡æ–™")
        return None

    # åŸ·è¡Œæ—¥æœŸè½‰æ›
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"])

    # JSON åªèªå¾—å­—ä¸²ï¼Œä¸èªå¾— Timestamp ç‰©ä»¶
    df["date"] = df["date"].astype(str)

    return df


# ===å®šç¾©ç‰¹å¾µå·¥ç¨‹çš„å‡½æ•¸


def add_time_features(df):
    """æ·»åŠ æ™‚é–“ç›¸é—œç‰¹å¾µ"""
    df = df.copy()
    df["year"] = df["date"].dt.year
    df["month"] = df["date"].dt.month
    df["dayofyear"] = df["date"].dt.dayofyear
    df["dayofweek"] = df["date"].dt.dayofweek
    return df


def compute_price_features(df, target_col="crop_price_per_kg", lookback=7):
    """è¨ˆç®—åƒ¹æ ¼ç›¸é—œçš„æ»¯å¾Œèˆ‡çµ±è¨ˆç‰¹å¾µ"""
    df = df.sort_values("date").copy()

    for lag in range(1, lookback + 1):
        df[f"price_lag{lag}"] = df[target_col].shift(lag)

    df["price_rolling_mean7"] = df[target_col].shift(1).rolling(7, min_periods=1).mean()
    df["price_rolling_std7"] = df[target_col].shift(1).rolling(7, min_periods=1).std()
    df["price_diff1"] = df[target_col].diff(1).shift(1)

    return df


def compute_weather_features(df, weather_cols):
    """è¨ˆç®—æ°£è±¡ç›¸é—œçš„æ»¯å¾Œç‰¹å¾µ"""
    df = df.sort_values("date").copy()

    for col in weather_cols:
        for lag in [1, 3, 7]:
            df[f"{col}_lag{lag}"] = df[col].shift(lag)
        df[f"{col}_rolling_mean7"] = df[col].shift(1).rolling(7, min_periods=1).mean()

    return df


def prepare_features(df, price_col="crop_price_per_kg"):
    """å®Œæ•´çš„ç‰¹å¾µæº–å‚™æµç¨‹"""
    weather_cols = [
        "station_pressure",
        "air_temperature",
        "relative_humidity",
        "wind_speed",
        "precipitation",
    ]

    df = add_time_features(df)
    df = compute_price_features(df, target_col=price_col, lookback=7)
    df = compute_weather_features(df, weather_cols=weather_cols)

    return df


# å®šç¾©é æ¸¬æœªä¾†åƒ¹æ ¼çš„å‡½æ•¸(ä½¿ç”¨éè¿´)


def create_new_row_with_features(
    current_data,
    future_date,
    predicted_price,
    price_col="crop_price_per_kg",
    weather_cols=None,
):
    """å»ºç«‹åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„æ–°è³‡æ–™åˆ—ï¼Œç”¨æ–¼éè¿´é æ¸¬"""
    if weather_cols is None:
        weather_cols = [
            "station_pressure",
            "air_temperature",
            "relative_humidity",
            "wind_speed",
            "precipitation",
        ]

    latest_row = current_data.iloc[-1]
    new_row = {}

    # åŸºæœ¬è³‡è¨Š
    new_row["date"] = future_date
    new_row["crop_id"] = latest_row["crop_id"]
    if "crop_index" in latest_row:
        new_row["crop_index"] = latest_row["crop_index"]
    new_row[price_col] = predicted_price

    # æ™‚é–“ç‰¹å¾µ
    new_row["year"] = future_date.year
    new_row["month"] = future_date.month
    new_row["dayofyear"] = future_date.dayofyear
    new_row["dayofweek"] = future_date.dayofweek

    # æ°£è±¡ç‰¹å¾µï¼ˆä½¿ç”¨æœ€æ–°å€¼ï¼‰
    for col in weather_cols:
        new_row[col] = latest_row[col]

    new_row["is_typhoon"] = 0
    new_row["typhoon_name"] = None

    # æ›´æ–°åƒ¹æ ¼æ»¯å¾Œç‰¹å¾µ
    for lag in range(1, 8):
        if lag == 1:
            new_row[f"price_lag{lag}"] = latest_row[price_col]
        else:
            new_row[f"price_lag{lag}"] = latest_row.get(
                f"price_lag{lag-1}", latest_row[price_col]
            )

    # æ›´æ–°åƒ¹æ ¼æ»¾å‹•çµ±è¨ˆç‰¹å¾µ
    recent_prices = []
    for lag in range(1, 8):
        key = f"price_lag{lag}"
        if key in latest_row and pd.notna(latest_row[key]):
            recent_prices.append(latest_row[key])

    if len(recent_prices) > 0:
        new_row["price_rolling_mean7"] = float(np.mean(recent_prices))
        new_row["price_rolling_std7"] = float(
            np.std(recent_prices) if len(recent_prices) > 1 else 0
        )
    else:
        new_row["price_rolling_mean7"] = latest_row[price_col]
        new_row["price_rolling_std7"] = 0.0

    new_row["price_diff1"] = predicted_price - latest_row[price_col]

    # æ›´æ–°æ°£è±¡æ»¯å¾Œç‰¹å¾µ
    for col in weather_cols:
        for lag in [1, 3, 7]:
            lag_col = f"{col}_lag{lag}"
            if lag == 1:
                new_row[lag_col] = latest_row[col]
            else:
                prev_lag_col = f"{col}_lag{lag-1}"
                new_row[lag_col] = latest_row.get(prev_lag_col, latest_row[col])

        # æ›´æ–°æ»¾å‹•å¹³å‡
        recent_weather = []
        for lag in range(1, 8):
            lag_col = f"{col}_lag{lag}"
            if lag_col in latest_row and pd.notna(latest_row[lag_col]):
                recent_weather.append(latest_row[lag_col])
            elif lag == 1:
                recent_weather.append(latest_row[col])

        if len(recent_weather) > 0:
            new_row[f"{col}_rolling_mean7"] = float(np.mean(recent_weather))
        else:
            new_row[f"{col}_rolling_mean7"] = latest_row[col]

    return new_row


def predict_future_recursive_optimized(
    historical_data,
    best_model,
    test_end_date,
    feature_cols,
    price_col="crop_price_per_kg",
    forecast_horizon=7,
):
    """éè¿´é æ¸¬æœªä¾†åƒ¹æ ¼"""
    current_data = historical_data.copy()
    future_predictions = []

    for day_offset in range(1, forecast_horizon + 1):
        future_date = test_end_date + timedelta(days=day_offset)

        # æº–å‚™ç‰¹å¾µ
        X_future = current_data.iloc[[-1]][feature_cols].fillna(0)
        predicted_price = best_model.predict(X_future)[0]

        # å»ºç«‹æ–°è³‡æ–™åˆ—
        new_row = create_new_row_with_features(
            current_data,
            future_date,
            predicted_price,
            price_col=price_col,
        )

        future_predictions.append(
            {
                "date": future_date,
                "crop_id": new_row["crop_id"],
                "price_prediction": predicted_price,
            }
        )

        # åŠ å…¥æ­·å²è³‡æ–™
        new_row_df = pd.DataFrame([new_row])
        current_data = pd.concat([current_data, new_row_df], ignore_index=True)

    return pd.DataFrame(future_predictions)


# ===å®šç¾©ä½¿ç”¨æ¨¡å‹çš„åƒæ•¸


# ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬
def predict_with_loaded_model(
    df, model, feature_cols, crop_to_index, price_col, test_days, forecast_horizon
):
    """ä½¿ç”¨å·²è¼‰å…¥çš„æ¨¡å‹é€²è¡Œé æ¸¬"""

    # è³‡æ–™æ¸…ç†èˆ‡ç‰¹å¾µæº–å‚™
    df_clean = df.dropna(subset=[price_col]).copy()
    df_clean = prepare_features(df_clean, price_col=price_col)

    # åªç§»é™¤é—œéµæ¬„ä½çš„ NaN
    df_clean = df_clean.dropna(subset=[price_col, "price_lag1"])

    # åŠ å…¥ crop_index
    df_clean["crop_index"] = df_clean["crop_id"].map(crop_to_index)
    df_clean = df_clean[df_clean["crop_index"].notna()]

    per_crop_data = {}
    for crop_id in sorted(df_clean["crop_id"].unique()):
        crop_data = (
            df_clean[df_clean["crop_id"] == crop_id]
            .sort_values("date")
            .reset_index(drop=True)
        )

        if len(crop_data) < test_days + 30:
            print(f"{crop_id}: è³‡æ–™ä¸è¶³ ({len(crop_data)} ç­† < {test_days + 30})")
            continue

        test_start_idx = len(crop_data) - test_days
        test_df = crop_data.iloc[test_start_idx:].copy()

        per_crop_data[crop_id] = {
            "test_df": test_df,
            "full_df": crop_data,
        }

    # é–‹å§‹é æ¸¬
    all_predictions = []

    for crop_id, data_dict in per_crop_data.items():
        test_df = data_dict["test_df"]
        full_df = data_dict["full_df"]

        # æ¸¬è©¦é›†é æ¸¬
        X_test = test_df[feature_cols].fillna(0)
        test_pred = model.predict(X_test)

        test_results = test_df[["date", "crop_id", price_col]].copy()
        test_results["price_prediction_global"] = test_pred
        test_results = test_results.rename(columns={price_col: "price_actual"})

        # æœªä¾†é æ¸¬
        test_end_date = test_df["date"].max()
        future_pred = predict_future_recursive_optimized(
            historical_data=full_df,
            best_model=model,
            test_end_date=test_end_date,
            feature_cols=feature_cols,
            price_col=price_col,
            forecast_horizon=forecast_horizon,
        )

        future_pred = future_pred.rename(
            columns={"price_prediction": "price_prediction_global"}
        )

        all_pred_crop = pd.concat([test_results, future_pred], ignore_index=True)
        all_predictions.append(all_pred_crop)

        print(f"{crop_id}: é æ¸¬å®Œæˆ")

    if len(all_predictions) == 0:
        return None

    final_predictions = pd.concat(all_predictions, ignore_index=True)
    return final_predictions


# å°‡é æ¸¬åƒ¹æ ¼è½‰æ›ç‚ºprice_predictionæ ¼å¼çš„å‡½æ•¸


def transform_to_price_prediction_format(df):
    """å°‡é æ¸¬çµæœæ ¼å¼è½‰æ›ç‚º price_prediction æ ¼å¼"""
    print("ğŸ”„ è½‰æ›ç‚º price_prediction æ ¼å¼...")

    actual_df = df[["crop_id", "date"]].copy()
    actual_df["mode"] = "actual"
    actual_df["price"] = df["price_actual"]

    prediction_df = df[["crop_id", "date"]].copy()
    prediction_df["mode"] = "prediction"
    prediction_df["price"] = df["price_prediction_global"]

    result_df = pd.concat([actual_df, prediction_df], ignore_index=True)
    result_df = result_df.sort_values(
        by=["crop_id", "date", "mode"], ascending=[True, True, True]
    ).reset_index(drop=True)

    print(f"âœ… è½‰æ›å®Œæˆï¼š{len(result_df)} ç­†è³‡æ–™")
    return result_df


# æ­£å¼è¼‰å…¥æ¨¡å‹çš„å‡½æ•¸
def load_model_artifacts():
    """è¼‰å…¥æ¨¡å‹å’Œç›¸é—œæª”æ¡ˆ"""

    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    required_files = [
        model_dir,
        model_path,
        metadata_path,
        feature_cols_path,
        crop_index_path,
    ]
    for file_path in required_files:
        if not Path(file_path).exists():
            raise FileNotFoundError(
                f"æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {file_path}\n" f"æŸ¥ç„¡æ¨¡å‹ï¼Œè«‹å…ˆè¨“ç·´æ¨¡å‹ï¼"
            )

    # è¼‰å…¥æ¨¡å‹
    model = joblib.load(model_path)

    # è¼‰å…¥ç‰¹å¾µæ¬„ä½
    feature_cols = joblib.load(feature_cols_path)

    # è¼‰å…¥ä½œç‰©å°æ‡‰è¡¨
    crop_to_index = joblib.load(crop_index_path)

    # è¼‰å…¥å…ƒæ•¸æ“š
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    return model, feature_cols, crop_to_index, metadata


#  é–‹å§‹è™•ç† Airflow DAG
@dag(
    dag_id="d_03_price_prediction_generator_upload_dag",
    description="æ¯æ—¥å¾MySQLåŸ·è¡Œviewå–å‡ºæ­·å²åƒ¹æ ¼ï¼Œä¸¦é€éæ¨¡å‹é æ¸¬è³‡æ–™å¾Œå›å‚³è‡³MySQL",
    schedule=None,
    start_date=pendulum.datetime(2020, 1, 1, tz="Asia/Taipei"),
    catchup=False,
    tags=["view", "etl", "mysql", "pymysql", "price", "prediction", "ML"],
    is_paused_upon_creation=False,
)
def price_prediction_generator_etl_dag_pymysql():

    @task
    def extract_price_data():
        """extract"""
        print("--- [Task 1] Extract: é–‹å§‹æŠ“å–è¿‘90å¤©åƒ¹æ ¼è³‡æ–™ ---")
        df = query_data_from_mysql(sql_template)

        # å°‡DataFrameè½‰ç‚º dict å‚³çµ¦ä¸‹ä¸€å€‹ task
        return df.to_dict(orient="records")

    @task
    def transform_price_data(raw_data: list):
        """transform"""
        print("--- [Task 2] Transform: é–‹å§‹è¼‰å…¥æ¨¡å‹ä¸¦é æ¸¬åƒ¹æ ¼ ---")
        if not raw_data:
            print("ç„¡è³‡æ–™å¯è™•ç†ã€‚")
            return []

        # å¾ XCom (List of Dicts) é‚„åŸå› DataFrame
        df = pd.DataFrame(raw_data)

        # å†å°‡æ™‚é–“è½‰å›timestampæ‰èƒ½åšç‰¹å¾µå·¥ç¨‹
        df["date"] = pd.to_datetime(df["date"])

        try:
            # è¼‰å…¥æ¨¡å‹
            model, feature_cols, crop_to_index, metadata = load_model_artifacts()

            # æ­£å¼é–‹å§‹åŸ·è¡Œé æ¸¬
            final_predictions = predict_with_loaded_model(
                df,
                model,
                feature_cols,
                crop_to_index,
                price_col="crop_price_per_kg",
                test_days=defaults["test_days"],
                forecast_horizon=defaults["forecast_horizon"],
            )

            if final_predictions is None:
                print("\né æ¸¬å¤±æ•—ï¼Œç¨‹å¼çµæŸ")

        except FileNotFoundError as e:
            print(f"\n{e}")
            print(f"æŸ¥ç„¡æ¨¡å‹ï¼Œè«‹å…ˆè¨“ç·´æ¨¡å‹ï¼")

        # ç¯©é¸å‡ºéœ€è¦æ¬„ä½
        df = final_predictions[
            ["date", "crop_id", "price_actual", "price_prediction_global"]
        ].copy()

        # å»é‡ï¼Œä¿ç•™æœ€å¾Œä¸€ç­†
        df = df.drop_duplicates(subset=["date", "crop_id"], keep="last")

        # é‡æ–°å‘½åæ¬„ä½
        df = df.rename(
            columns={
                "price_actual": "actual",
                "price_prediction_global": "prediction",
            }
        )

        # åˆä½µæ¬„ä½(å¯¬è½‰é•·)
        df = df.melt(id_vars=["date", "crop_id"], var_name="mode", value_name="price")

        # ä¾ç…§crop_idåŠdateé‡æ–°æ’åº
        df = df.sort_values(["crop_id", "date"]).reset_index(drop=True)

        # JSON åªèªå¾—å­—ä¸²ï¼Œä¸èªå¾— Timestamp ç‰©ä»¶
        df["date"] = df["date"].astype(str)

        # å†æ¬¡è½‰ç‚º Dict List å›å‚³çµ¦ Load Task
        return df.to_dict(orient="records")

    @task
    def load_price_data(transformed_data: list):
        """load"""
        print("--- [Task 3] Load: é–‹å§‹å¯«å…¥è³‡æ–™åº« ---")
        if not transformed_data:
            print("ç„¡è³‡æ–™å¯å¯«å…¥ã€‚")
            return

        # å¾ XCom (List of Dicts) é‚„åŸå› DataFrame
        df = pd.DataFrame(transformed_data)

        # MySQLä¸æ¥å—Nanï¼Œæ•…è¦å…ˆè½‰ç‚ºNone
        df = df.replace({np.nan: None})

        # æŒ‡å®šè¦åŒ¯å…¥çš„è¡¨æ ¼
        db_table = "price_prediction"

        # æ­£å¼é–‹å§‹åŒ¯å…¥
        try:
            # A. å°‡ DataFrame è½‰æ›ç‚º(tuple)
            data_tuples = [tuple(row) for row in df.to_numpy()]

            # B. æº–å‚™ SQL æ’å…¥æ¨¡æ¿
            sql_template = f"""
                INSERT INTO {db_table} ({", ".join(list(df.columns))})
                VALUES ({", ".join(["%s"] * len(df.columns))})
                ON DUPLICATE KEY UPDATE
                    `price` = VALUES(`price`);
            """
            conn = pymysql.connect(**db_config)
            cursor = conn.cursor()

            # C. åŸ·è¡Œã€Œä¸€æ¬¡æ€§ã€æ‰¹æ¬¡æ’å…¥
            print("é–‹å§‹åŸ·è¡Œ executemany...")
            cursor.executemany(sql_template, data_tuples)

            # D. ç²å–æ’å…¥æˆåŠŸçš„ç­†æ•¸
            cursor.execute("SELECT ROW_COUNT()")
            successful_inserts = cursor.fetchone()[0]

            # E. æäº¤äº¤æ˜“
            conn.commit()
            print(f"æˆåŠŸå°‡ {successful_inserts} ç­†è³‡æ–™å¯«å…¥ '{db_table}' è³‡æ–™è¡¨ã€‚")

        except Exception as e:
            conn.rollback()
            print(f"å¯«å…¥è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        finally:
            cursor.close()
            conn.close()

    trigger_next_dag = TriggerDagRunOperator(
        task_id="trigger_dag_04",
        trigger_dag_id="d_04_insert_to_gsheet_dag",
        wait_for_completion=False,  # ä¸ç­‰ç¢ºèªä¸‹ä¸€å€‹dagåŸ·è¡Œå°±çµæŸä»»å‹™
    )

    # è¨­å®šç›¸ä¾æ€§
    raw_data = extract_price_data()
    clean_data = transform_price_data(raw_data)
    load_price_data(clean_data) >> trigger_next_dag


price_prediction_generator_etl_dag_pymysql()
